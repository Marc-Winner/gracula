{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-community langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS = 'Yjg4MTQzMmUtNDAwMS00NDk0LThjOGUtNmU5ZWQ2YzQ4NDQ2OmQ4MWMxZGZiLTFmNGYtNDk5NS05OGQzLTBiMzYyYWJmNjk3OA=='\n",
    "TESTPDF = \"../data/papers/10.1002@solr.201900061.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import GigaChatEmbeddings\n",
    "from langchain.agents import AgentExecutor, ZeroShotAgent\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.chat_models import GigaChat\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from PyPDF2 import PdfReader\n",
    "# from langchain_community.document_loaders import TextLoader, PDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "def extract_raw_text_from_pdf(path) -> str:\n",
    "    reader = PdfReader(stream=path)\n",
    "\n",
    "    raw_text = ''\n",
    "    for _, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            raw_text += text\n",
    "\n",
    "    return raw_text\n",
    "\n",
    "def get_index_from_pdf(pdf_path) -> Any:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    raw_text = extract_raw_text_from_pdf(path=pdf_path)\n",
    "    texts = text_splitter.split_text(text=raw_text)\n",
    "    embeddings = GigaChatEmbeddings(credentials=CREDENTIALS, verify_ssl_certs=False, scope='GIGACHAT_API_CORP')\n",
    "    index = FAISS.from_texts(texts=texts, embedding=embeddings)\n",
    "    return index\n",
    "\n",
    "def invoke_chain_with_index(chain, index, query) -> dict:\n",
    "    query = \"For each of these {Spiro HTM, Spiro-CB, Spiro-THF} report efficiency (PCE or optimized efficiency or Î·). Put that data in the markdown table with columns 'HTM' - 'PCE'\"\n",
    "    docs = index.similarity_search(query)\n",
    "    return chain.invoke({\"input_documents\": docs, \"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/papers/2014/2014_1.pdf\"\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "CONTEXT_BASED_Q_TEMPLATE = \"\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "def get_doc_chain(llm, template, prompt):\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    return create_stuff_documents_chain(llm, prompt)\n",
    "def get_retriever(index, doc_chain):\n",
    "    return index.as_retriever()\n",
    "def get_retrieval_chain(index, doc_chain):\n",
    "    retriever = index.as_retriever()\n",
    "    return create_retrieval_chain(retriever, doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    get_retriever,\n",
    "    \"state-of-union-retriever\",\n",
    "    \"Query a retriever to get information about state of the union address\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Final response to the question being asked\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"The final answer to respond to the user\")\n",
    "    sources: List[int] = Field(\n",
    "        description=\"List of page chunks that contain answer to the question. Only include a page chunk if it contains relevant information\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.agents import AgentActionMessageLog, AgentFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(output):\n",
    "    # If no function was invoked, return to user\n",
    "    if \"function_call\" not in output.additional_kwargs:\n",
    "        return AgentFinish(return_values={\"output\": output.content}, log=output.content)\n",
    "\n",
    "    # Parse out the function call\n",
    "    function_call = output.additional_kwargs[\"function_call\"]\n",
    "    name = function_call[\"name\"]\n",
    "    inputs = json.loads(function_call[\"arguments\"])\n",
    "\n",
    "    # If the Response function was invoked, return to the user with the function inputs\n",
    "    if name == \"Response\":\n",
    "        return AgentFinish(return_values=inputs, log=str(function_call))\n",
    "    # Otherwise, return an agent action\n",
    "    else:\n",
    "        return AgentActionMessageLog(\n",
    "            tool=name, tool_input=inputs, log=\"\", message_log=[output]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models.gigachat import GigaChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "giga = GigaChat(credentials=CREDENTIALS, \n",
    "                verify_ssl_certs=False,\n",
    "                scope='GIGACHAT_API_CORP',\n",
    "                model=\"GigaChat-Pro\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = giga.bind_functions([retriever_tool, Response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        # Format agent scratchpad from intermediate steps\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | parse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(tools=[retriever_tool], agent=agent, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke( # this code causes error 422 when executed\n",
    "    {\"input\": \"Did the device reach the PCE over 35%?\"},\n",
    "    return_only_outputs=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigachat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
